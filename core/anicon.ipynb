{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T12:17:17.356670Z",
     "start_time": "2021-01-25T12:17:17.349142Z"
    },
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim, autograd\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets, utils\n",
    "\n",
    "from model import Generator, Discriminator\n",
    "from dataset import PortraitDataset\n",
    "\n",
    "device = 'cpu'\n",
    "path = '../datasets/danbooru-images/portrait/'  # path to the dataset\n",
    "iteration = 960                                 # total training iterations\n",
    "batch = 16                                      # batch size\n",
    "n_sample = 16                                   # number of the samples generated during training\n",
    "img_size = 128                                  # image sizes for the model\n",
    "lb_size = 23                                    # label sizes for the model\n",
    "r1 = 10                                         # weight of the r1 regularization\n",
    "path_regularize = 2                             # weight of the path length regularization\n",
    "path_batch_shrink = 2                           # batch size reducing factor for the path length regularization\n",
    "                                                # (reduce memory consumption)\n",
    "d_reg_every = 16                                # interval of the applying r1 regularization\n",
    "g_reg_every = 4                                 # interval of the applying path length regularization\n",
    "mixing = 0.9                                    # probability of latent code mixing\n",
    "ckpt = None                                     # path to the checkpoints to resume training\n",
    "#ckpt = f'checkpoint/{str(100).zfill(6)}.pt'\n",
    "lr = 0.002                                      # learning rate\n",
    "channel_multiplier = 1                          # channel multiplier factor for the model\n",
    "latent = 128\n",
    "n_mlp = 6\n",
    "start_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T12:21:06.004094Z",
     "start_time": "2021-01-25T12:21:05.877163Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
    "\n",
    "def sample_data(loader):\n",
    "    while True:\n",
    "        for img_batch, lb_batch in loader:\n",
    "            yield img_batch, lb_batch\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "def make_noise(batch, latent_dim, n_noise, device):\n",
    "    if n_noise == 1:\n",
    "        return torch.randn(batch, latent_dim, device=device)\n",
    "    noises = torch.randn(n_noise, batch, latent_dim, device=device).unbind(0)\n",
    "    return noises\n",
    "\n",
    "def mixing_noise(batch, latent_dim, prob, device):\n",
    "    if prob > 0 and random.random() < prob:\n",
    "        return make_noise(batch, latent_dim, 2, device)\n",
    "    else:\n",
    "        return [make_noise(batch, latent_dim, 1, device)]\n",
    "\n",
    "def d_logistic_loss(real_pred, fake_pred):\n",
    "    real_loss = F.softplus(-real_pred)\n",
    "    fake_loss = F.softplus(fake_pred)\n",
    "    return real_loss.mean() + fake_loss.mean()\n",
    "\n",
    "def d_r1_loss(real_pred, real_img):\n",
    "    grad_real, = autograd.grad(\n",
    "        outputs=real_pred.sum(), inputs=real_img, create_graph=True\n",
    "    )\n",
    "    grad_penalty = grad_real.pow(2).reshape(grad_real.shape[0], -1).sum(1).mean()\n",
    "    return grad_penalty\n",
    "\n",
    "def g_nonsaturating_loss(fake_pred):\n",
    "    loss = F.softplus(-fake_pred).mean()\n",
    "    return loss\n",
    "\n",
    "def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01):\n",
    "    noise = torch.randn_like(fake_img) / math.sqrt(\n",
    "        fake_img.shape[2] * fake_img.shape[3]\n",
    "    )\n",
    "    grad, = autograd.grad(\n",
    "        outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True\n",
    "    )\n",
    "    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n",
    "    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)\n",
    "    path_penalty = (path_lengths - path_mean).pow(2).mean()\n",
    "    return path_penalty, path_mean.detach(), path_lengths\n",
    "\n",
    "with open('sample_labels.json', 'r') as f:\n",
    "    sample_labels = json.load(f)\n",
    "total_samples = len(sample_labels)\n",
    "def get_random_labels(batch, device):\n",
    "    labels = []\n",
    "    for i in range(batch):\n",
    "        labels.append(sample_labels[random.randint(0, total_samples - 1)])\n",
    "    return torch.Tensor(labels).float().to(device)\n",
    "\n",
    "def train(loader, generator, discriminator, g_optim, d_optim, g_ema, device):\n",
    "    loader = sample_data(loader)\n",
    "    \n",
    "    pbar = range(iteration)\n",
    "    pbar = tqdm(pbar, initial=start_iter, dynamic_ncols=True, smoothing=0.01)\n",
    "    \n",
    "    mean_path_length = 0\n",
    "\n",
    "    d_loss_val = 0\n",
    "    r1_loss = torch.tensor(0.0, device=device)\n",
    "    g_loss_val = 0\n",
    "    path_loss = torch.tensor(0.0, device=device)\n",
    "    path_lengths = torch.tensor(0.0, device=device)\n",
    "    mean_path_length_avg = 0\n",
    "    loss_dict = {}\n",
    "    \n",
    "    g_module = generator\n",
    "    d_module = discriminator\n",
    "    \n",
    "    accum = 0.5 ** (32 / (10 * 1000))\n",
    "\n",
    "    sample_z = torch.randn(n_sample, latent, device=device)\n",
    "    sample_l = get_random_labels(n_sample, device=device)\n",
    "    \n",
    "    for idx in pbar: \n",
    "        i = idx + start_iter\n",
    "\n",
    "        if i > iteration:\n",
    "            print('Done!')\n",
    "            break\n",
    "\n",
    "        real_img, real_lb = next(loader)\n",
    "        real_img = real_img.to(device)\n",
    "        real_lb = real_lb.to(device)\n",
    "\n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator, True)\n",
    "\n",
    "        noise = mixing_noise(batch, latent, mixing, device)\n",
    "        fake_lb = get_random_labels(batch, device)\n",
    "        fake_img, _ = generator(noise, fake_lb)\n",
    "        \n",
    "        fake_pred = discriminator(fake_img, fake_lb)\n",
    "        real_pred = discriminator(real_img, real_lb)\n",
    "        d_loss = d_logistic_loss(real_pred, fake_pred)\n",
    "\n",
    "        loss_dict['d'] = d_loss\n",
    "        loss_dict['real_score'] = real_pred.mean()\n",
    "        loss_dict['fake_score'] = fake_pred.mean()\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optim.step()\n",
    "\n",
    "        d_regularize = i % d_reg_every == 0\n",
    "\n",
    "        if d_regularize:\n",
    "            real_img.requires_grad = True\n",
    "            real_pred = discriminator(real_img, real_lb)\n",
    "            r1_loss = d_r1_loss(real_pred, real_img)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            (r1 / 2 * r1_loss * d_reg_every + 0 * real_pred[0]).backward()\n",
    "\n",
    "            d_optim.step()\n",
    "\n",
    "        loss_dict['r1'] = r1_loss\n",
    "\n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(discriminator, False)\n",
    "\n",
    "        noise = mixing_noise(batch, latent, mixing, device)\n",
    "        fake_lb = get_random_labels(batch, device)\n",
    "        fake_img, _ = generator(noise, fake_lb)\n",
    "        fake_pred = discriminator(fake_img, fake_lb)\n",
    "        g_loss = g_nonsaturating_loss(fake_pred)\n",
    "\n",
    "        loss_dict['g'] = g_loss\n",
    "\n",
    "        generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        g_regularize = i % g_reg_every == 0\n",
    "\n",
    "        if g_regularize:\n",
    "            path_batch_size = max(1, batch // path_batch_shrink)\n",
    "            noise = mixing_noise(\n",
    "                path_batch_size, latent, mixing, device\n",
    "            )\n",
    "            fake_lb = get_random_labels(path_batch_size, device)\n",
    "            fake_img, latents = generator(noise, fake_lb, return_latents=True)\n",
    "\n",
    "            path_loss, mean_path_length, path_lengths = g_path_regularize(\n",
    "                fake_img, latents, mean_path_length\n",
    "            )\n",
    "\n",
    "            generator.zero_grad()\n",
    "            weighted_path_loss = path_regularize * g_reg_every * path_loss\n",
    "\n",
    "            if path_batch_shrink:\n",
    "                weighted_path_loss += 0 * fake_img[0, 0, 0, 0]\n",
    "\n",
    "            weighted_path_loss.backward()\n",
    "\n",
    "            g_optim.step()\n",
    "\n",
    "            mean_path_length_avg = (mean_path_length.item())\n",
    "\n",
    "        loss_dict['path'] = path_loss\n",
    "        loss_dict['path_length'] = path_lengths.mean()\n",
    "\n",
    "        accumulate(g_ema, g_module, accum)\n",
    "\n",
    "        d_loss_val = loss_dict['d'].mean().item()\n",
    "        g_loss_val = loss_dict['g'].mean().item()\n",
    "        r1_val = loss_dict['r1'].mean().item()\n",
    "        path_loss_val = loss_dict['path'].mean().item()\n",
    "        real_score_val = loss_dict['real_score'].mean().item()\n",
    "        fake_score_val = loss_dict['fake_score'].mean().item()\n",
    "        path_length_val = loss_dict['path_length'].mean().item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            (\n",
    "                f'd: {d_loss_val:.4f}; g: {g_loss_val:.4f}; r1: {r1_val:.4f}; '\n",
    "                f'path: {path_loss_val:.4f}; mean path: {mean_path_length_avg:.4f}'\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        if wandb and args.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    'Generator': g_loss_val,\n",
    "                    'Discriminator': d_loss_val,\n",
    "                    'R1': r1_val,\n",
    "                    'Path Length Regularization': path_loss_val,\n",
    "                    'Mean Path Length': mean_path_length,\n",
    "                    'Real Score': real_score_val,\n",
    "                    'Fake Score': fake_score_val,\n",
    "                    'Path Length': path_length_val,\n",
    "                }\n",
    "            )\n",
    "        '''\n",
    "        if i % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                #g_ema.eval()\n",
    "                sample, _ = generator([sample_z], sample_l)\n",
    "                utils.save_image(\n",
    "                    sample,\n",
    "                    f'sample/{str(i).zfill(6)}.png',\n",
    "                    nrow=int(n_sample ** 0.5),\n",
    "                    normalize=True,\n",
    "                    range=(-1, 1),\n",
    "                )\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    'g': g_module.state_dict(),\n",
    "                    'd': d_module.state_dict(),\n",
    "                    'g_ema': g_ema.state_dict(),\n",
    "                    'g_optim': g_optim.state_dict(),\n",
    "                    'd_optim': d_optim.state_dict(),\n",
    "                },\n",
    "                f'checkpoint/{str(i).zfill(6)}.pt',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:10:49.046404Z",
     "start_time": "2021-01-25T11:10:47.802841Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    img_size,\n",
    "    lb_size,\n",
    "    latent,\n",
    "    n_mlp,\n",
    "    channel_multiplier=channel_multiplier\n",
    ").to(device)\n",
    "discriminator = Discriminator(\n",
    "    img_size,\n",
    "    lb_size,\n",
    "    channel_multiplier=channel_multiplier\n",
    ").to(device)\n",
    "g_ema = Generator(\n",
    "    img_size,\n",
    "    lb_size,\n",
    "    latent,\n",
    "    n_mlp,\n",
    "    channel_multiplier=channel_multiplier\n",
    ").to(device)\n",
    "g_ema.eval()\n",
    "accumulate(g_ema, generator, 0)\n",
    "\n",
    "g_reg_ratio = g_reg_every / (g_reg_every + 1)\n",
    "d_reg_ratio = d_reg_every / (d_reg_every + 1)\n",
    "\n",
    "g_optim = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=lr * g_reg_ratio,\n",
    "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
    ")\n",
    "d_optim = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=lr * d_reg_ratio,\n",
    "    betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n",
    ")\n",
    "\n",
    "if ckpt is not None:\n",
    "    print('load model:', ckpt)\n",
    "\n",
    "    args_ckpt = ckpt\n",
    "    ckpt = torch.load(args_ckpt, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    try:\n",
    "        ckpt_name = os.path.basename(args_ckpt)\n",
    "        start_iter = int(os.path.splitext(ckpt_name)[0])\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    generator.load_state_dict(ckpt['g'])\n",
    "    discriminator.load_state_dict(ckpt['d'])\n",
    "    g_ema.load_state_dict(ckpt['g_ema'])\n",
    "\n",
    "    g_optim.load_state_dict(ckpt['g_optim'])\n",
    "    d_optim.load_state_dict(ckpt['d_optim'])\n",
    "    \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = PortraitDataset(\n",
    "    path,\n",
    "    transform\n",
    ")\n",
    "loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T13:38:44.036273Z",
     "start_time": "2021-01-25T12:21:08.537720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d: 1.1145; g: 1.8890; r1: 0.0072; path: 0.0034; mean path: 0.2042: : 961it [1:17:35,  7.75s/it]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_iter = 360\n",
    "train(loader, generator, discriminator, g_optim, d_optim, g_ema, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T10:45:26.997832Z",
     "start_time": "2021-01-25T10:45:26.991606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters number of generator: 1195277\n",
      "parameters number of discriminator: 1210000\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('parameters number of generator:', count_parameters(generator))\n",
    "print('parameters number of discriminator:', count_parameters(discriminator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T14:03:16.404749Z",
     "start_time": "2021-01-25T14:03:15.833336Z"
    }
   },
   "outputs": [],
   "source": [
    "z = torch.randn(n_sample, latent, device=device)\n",
    "l = get_random_labels(n_sample, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean_latent = g_ema.mean_latent(4096, get_random_labels(4096, device=device))\n",
    "    g_ema.eval()\n",
    "    sample, _ = g_ema(\n",
    "        [z], l, truncation=0.7, truncation_latent=mean_latent\n",
    "    )\n",
    "    utils.save_image(\n",
    "        sample,\n",
    "        f'sample/{str(80000).zfill(6)}.png',\n",
    "        nrow=int(n_sample ** 0.5),\n",
    "        normalize=True,\n",
    "        range=(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T11:13:37.761738Z",
     "start_time": "2021-01-25T11:13:37.738671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
